<!---
;;
;;  This softeware is Copyright (c) 2009 A.F. Haffmans 
;;
;;    This file is part of cl-bliky.
;;
;;    cl-bliky is free software: you can redistribute it and/or modify
;;    it under the terms of the GNU General Public License as published by
;;   the Free Software Foundation, either version 3 of the License, or
;;    (at your option) any later version.
;;
;;    cl-bliky is distributed in the hope that it will be useful,
;;    but WITHOUT ANY WARRANTY; without even the implied warranty of
;;    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
;;    GNU General Public License for more details.
;;
;;    You should have received a copy of the GNU General Public License
;;    along with cl-bliky.  If not, see <http://www.gnu.org/licenses/>.
;;
;;

--->

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

<html>
  <head>
    <title> Mohegan SkunkWorks  </title>
    <link rel="stylesheet" href="/style.css" type="text/css"/>
  </head>  

  <body>
    <div id="blog-title">
      <a href="/"> <h1> Mohegan SkunkWorks </h1> </a> 
    </div>
    
    <div id="mainClm">
      <div id="post">
	<div id="post-type" style="visibility:hidden">
	  <p>"post"</p>
	</div>
	<div id="post-timestamp" >
	  <p>Sun, 01 Nov 2009 10:30:37 EST </p>
	</div>
	<div id="post-header">
	  <h2> Toy Problem: Simple One Dimensional Least Squares Learner.  </h2>
	</div>
	<div id="post-intro">
	  <P>In chapter two of Hastie, Tibshirani and Friedman 's  <A HREF="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">'The Elements of Statistical Learning'</A>  the authors discuss the use of least- squares regression to construct a data classifier for linearly separable data. <BR> <BR>  A set of training data together with the least-squares method is used to construct a hyper-plane in the data space. The classification of a data point depends on what side of the hyper-plane you end up on.<BR> <BR>  The example in Hastie uses two data classes in a two dimensional parameter space. I didn't grok the the example immediately, and I thought it would be helpful to try to construct my own much simpler example by staying in one dimension and using a simple normal distribution. The rest of this post describes the details. </P>
	</div>
	<div id="post-body">
	  <H2>Data Classes and Least Squares</H2><P>My data points are generated by two <A HREF="http://en.wikipedia.org/wiki/Normal_distribution">normal distributions.</A> with means on the interval [0,1].<BR> <BR> Class 1 is classified as -1 and class 2 as 1. In my example, class 1 will always have the smaller mean, and hence will lie to the left of class 2. As in  Hastie <A HREF="http://en.wikipedia.org/wiki/Linear_regression">linear regression.</A> is used to find a linear classifier for a set of training data for these two classes.<BR> <BR>  The data space is one-dimensional and the classification boundary will be a point on the interval [0,1] somewhere between the means of the two data classes. For a one dimensional data space linear regression models the classification response function as a line:     y = Mx + Q </P><P>The RSS is the sum of the squares of the difference between the actual and response predicted by the LSM equation : </P><PRE><CODE>RSS = SUM ( (y_obs - y_calc) ^2 ) </CODE></PRE><P>LSM tries to find the slope M and intercept Q such that the responses  generated by the least-squares model (LSM) have the smallest 'residual sum of squared errors' (RSS). </P><P><EM>x</EM> represents the input and is going to be generated by the normal distributions associated with one of the two data classes. <EM>y</EM> represents the response and the only values are -1 or 1, depending on whether <EM>x</EM> was generated by the first or second class distribution respectively. </P><P>The observed classifications, <EM>y_obs</EM>, can have only one of two values : -1 or 1. The values generated by the LSM, <EM>y_calc</EM>, range from Q (for <EM>x</EM> = 0) to M + Q (for <EM>x</EM> = 1). </P><P>The LSM separates the space into two pieces, one for one class and one for the other. In this case the data space is the line piece [0,1], and the 'hyper-plane' separating that space is where the linear regression line crosses the data space, yielding a classification boundary of : </P><PRE><CODE> S = -Q / M </CODE></PRE><P>The LSM estimate of the classifier is then : </P><PRE><CODE>x &lt; S -&gt; class 1  
x &gt; S -&gt; class 2  
</CODE></PRE><H2>A simple R script</H2><P>I've put together a quick <A HREF="http://www.r-project.org/">R script</A>  implementing these ideas. </P><P>This [R script]( http://github.com/fons/blog-code/blob/master/1d-classification-toy/sl-1d-regression.R) has two main functions : <EM>train</EM> and <EM>sim</EM>. The script is loaded on the R command prompt like so: </P><PRE><CODE>source('sl-1d-regression.R') </CODE></PRE><P>The <EM>train</EM> function is used to generate a graph showing two data classes as well as the line generated by the LSM separating both classes. The example discussed below was generated using train as follows : </P><PRE><CODE>&gt; train(20, 0.35, 0.15, 0.65, 0.15)  
  coefficients :  -1.916889  3.973720  
  classifier :  0.4824  
  prob of misclassification of class 1 :  0.16  
  prob of misclassification of class 2 :  0.16  
 
</CODE></PRE><P>The <EM>sim</EM> function returns a vector of monte-carlo simulations of the separation boundary. For example, here's  10 values of the boundary generated by <EM>sim</EM> : </P><PRE><CODE>&gt; sim(10, 0.35, 0.15, 0.65, 0.15)  
  (Intercept) (Intercept) (Intercept) (Intercept) (Intercept) (Intercept)  
   0.4495022   0.5185030   0.4978533   0.5887321   0.4547277   0.5029925  
  (Intercept) (Intercept) (Intercept) (Intercept)  
   0.4792027   0.5665940   0.4952061   0.5337139 </CODE></PRE><P>The following plots the density function for 400 values of the classification boundary, for the same class parameters as the <EM>train</EM> example above. </P><PRE><CODE>plot(density(sim(400, 0.15, 0.35, 0.65, 0.15)), xlim=c(0,1), xlab="", ylab="")  
</CODE></PRE><P>Obviously, since these  are monte-carlo simulations, subsequent runs are going to be different as the random values generated by <EM>rnorm</EM> are going to be different each time the function is run. </P><H2>Classification Results</H2><P><A NAME="narrow1d"><IMG SRC="http://github.com/fons/blog-images/raw/master/20091101/narrow1d.png"></A></P><P><A HREF="" NARROW1D="narrow1d">This graph</A> shows two classes and the resulting least-squares model. Both distributions have the same standard deviation of 0.15. One class marked the with red inverted triangles has a mean of 0.35.  The other class is marked with the blue squares and has a mean of 0.65. The size of each training class is 20. </P><P>Each class shows up twice. Once as part of the x-axis which is the data space for this problem. The second time I show them as data points (x,y) used in the linear regression. Obviously the red triangles all have y = -1, and the blue squares all have y = 1. </P><P>As you can see, there is some overlap between the two data sets, because of the variance of the normal distribution. </P><P>The green line represents the best fit of the data points according to the least-squares model (LSM). The classification boundary is the point where the green line crosses the x-axis. As you can see that's somewhere around 0.5. </P><P>You would expect 0.5 to be a good estimate of the classification boundary S because the normal distribution is symmetric around the mean and both classes are equi-distant from 0.5. </P><P>The R-script generates the probability of misclassification defined as the probability of ending up on the wrong side of the estimated classification boundary : </P><PRE><CODE> S_est = 0.5 * (mean_1 + mean_2)  
 
 P_mis_classified = Prob( x &gt; S_est) for class 1  
 		      = Prob( x &lt; S_est) for class 2  
</CODE></PRE><P>This is obviously determined in large part by the variance (for given mean), and for this example the probability is around 16 % for both classes. </P><P>The <EM>sim</EM> can be used to generate a whole set of estimates of the classification boundary. This is in fact sampling the space of estimators. The mean of this sample should be very close to the actual value thanks to the <A HREF="http://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem.</A> Here's a typical run </P><PRE><CODE> &gt;mean(sim(400, 0.35, 0.15, 0.65, 0.15))  
  [1] 0.4977804  
&gt; mean(sim(400, 0.35, 0.15, 0.65, 0.15))  
  [1] 0.4990484  
&gt; mean(sim(400, 0.35, 0.15, 0.65, 0.15))  
  [1] 0.5024554 </CODE></PRE><P>The R function <EM>mean</EM> is used to find the mean of the values of the vector of simulation results. </P><P><A NAME="samplemean1d"><IMG SRC="http://github.com/fons/blog-images/raw/master/20091101/samplemean1d.png"></A></P><P><A HREF="" SAMPLEMEAN1D="samplemean1d">This graph</A> shows a plot of the density of the sample distribution for the mean. The graph shows two distributions one for a standard deviations 0.15 in blue and 0.35 in red respectively. </P><P>Notice how wide the distribution for sd=0.35 is around 0.5. It turns out that about 33 % of your data will be misclassified because there will be significant overlap between the two classes. </P><P><A NAME="wide1d"><IMG SRC="http://github.com/fons/blog-images/raw/master/20091101/wide1d.png"></A></P><P><A HREF="" WIDE1D="wide1d">This graph</A> is generated using a standard deviation of 0.35 for both classes and with 50 data points in each class. As you can see there's significant overlap between the two classes. In fact, it would be hard to just visually pick out a good classification boundary. </P> 
	</div>
	<div id="post-timestamp-id" style="visibility:hidden">
	  <p>3466078237 </p>
	</div>
      </div>

      <div id="footer">
	<p>
	  Powered by <a href="http://www.github.com/fons/cl-bliky"> cl-bliky </a>
	</p>
      </div>
      
    </div><!-- end of main area -->

  </body>
</html>
